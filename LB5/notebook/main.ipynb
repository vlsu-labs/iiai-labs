{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classes and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "USED_DIR_SIZE = 16\n",
    "LEAR_DATA_PATH = f'C:\\my\\study\\IIAI\\LB5\\dataset_root\\{USED_DIR_SIZE}'\n",
    "TEST_DATA_PATH = f'C:\\my\\study\\IIAI\\LB5\\input\\{USED_DIR_SIZE}'\n",
    "MODEL_INSTANCE_PATH = 'model_state.pth'\n",
    "CLASSES = ['А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ё', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я']      \n",
    "SIMPLE_CLASSES = ['А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я']      \n",
    "\n",
    "LETTERS = SIMPLE_CLASSES if IMAGE_SIZE == 16 else CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToModelFormatConverter:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def process_single_image(self, path: str):\n",
    "        image = Image.open(path)\n",
    "        return self.transform(image).unsqueeze(0)  # Add batch dimension \n",
    "\n",
    "\n",
    "    def process_folder(self, folder_path: str):\n",
    "        return ImageFolder(root=folder_path, transform=self.transform)\n",
    "\n",
    "\n",
    "class FromMultipleTestDataSet:\n",
    "    def __init__(self):\n",
    "        self.converter = ImageToModelFormatConverter()\n",
    "        self.pattern = pattern = re.compile(r'(.+)\\.(jpg|png)$', re.IGNORECASE)\n",
    "    \n",
    "    def form_dataset(self, root_path: str, noise_level: float = 0) -> Dict['str', Dict['str', List]]:\n",
    "        multiple_data = {}\n",
    "\n",
    "        dirs = [f for f in listdir(root_path) if not isfile(join(root_path, f))]\n",
    "        \n",
    "\n",
    "        for dir in dirs:\n",
    "            data = {}\n",
    "            path = join(root_path, dir)\n",
    "\n",
    "            for file in listdir(path):\n",
    "                if (match := self.pattern.match(file)):\n",
    "                    name = match.group(1)\n",
    "                    \n",
    "                    if name not in data:\n",
    "                        data[name] = []\n",
    "                    \n",
    "                    data[name] = self.converter.process_single_image(join(path, file))         \n",
    "\n",
    "                    if noise_level != 0:\n",
    "                        noise = torch.randn_like(data[name]) * noise_level\n",
    "                        noisy_image_tensor = data[name] + noise\n",
    "                        noisy_image_tensor = torch.clamp(noisy_image_tensor, 0, 1)\n",
    "                        data[name] = noisy_image_tensor\n",
    "\n",
    "            multiple_data[dir] = data\n",
    "\n",
    "        return multiple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LetterRecognitionModel, self).__init__()\n",
    "        var = int(IMAGE_SIZE / 4)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Changed input channels to 1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * var * var, 256)  # Adjusted input size based on new image size\n",
    "        self.fc2 = nn.Linear(256, len(LETTERS))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = int(IMAGE_SIZE / 4)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * var * var)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "converter = ImageToModelFormatConverter()\n",
    "\n",
    "dataloader = DataLoader(converter.process_folder(LEAR_DATA_PATH), batch_size=32, shuffle=True)\n",
    "\n",
    "model = LetterRecognitionModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "iterations = 333\n",
    "for iter in range(iterations):\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Iteration {iter+1}/{iterations}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODEL_INSTANCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots_adjust\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'model' not in locals():\n",
    "    model = LetterRecognitionModel()\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_INSTANCE_PATH))\n",
    "model.eval()\n",
    "\n",
    "tests_loader = FromMultipleTestDataSet()\n",
    "used_data_set = 'complex'\n",
    "\n",
    "\n",
    "\n",
    "noises = np.arange(0, 0.9+0.1, 0.1)\n",
    "\n",
    "prediction_data = {}\n",
    "\n",
    "for level in noises:\n",
    "    count = 0\n",
    "    match = 0\n",
    "    for letter, data in tests_loader.form_dataset(TEST_DATA_PATH, level)[used_data_set].items():\n",
    "        for test in data:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(test)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "            predicted_class = torch.argmax(output).item()\n",
    "\n",
    "            predicted_class = torch.argmax(output).item()\n",
    "\n",
    "            predicted_letter = LETTERS[predicted_class]\n",
    "        \n",
    "            match += 1 if predicted_letter == letter else 0 \n",
    "\n",
    "            print(f\"Noise: {level} Actual: {letter} Predicted: {predicted_letter} Check: {letter==predicted_letter}\")\n",
    "    \n",
    "    prediction_data[f'{level}'] = (match, count)\n",
    "\n",
    "\n",
    "fig = plt.figure(num=1, facecolor='w', edgecolor='k')\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(5)\n",
    "subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(\n",
    "    prediction_data.keys(), \n",
    "    list(map(lambda x: float(x[0])/x[1] ,prediction_data.values())), \n",
    "    color='red', \n",
    "    linestyle='solid', \n",
    "    marker='o'\n",
    ")\n",
    "ax.set_title(\n",
    "    f\"Model accuracuty for '{used_data_set}' on image: {IMAGE_SIZE}x{IMAGE_SIZE}\" \\\n",
    "        if IMAGE_SIZE == USED_DIR_SIZE else \\\n",
    "        f\"Model accuracuty for '{used_data_set}' on image: {IMAGE_SIZE}x{IMAGE_SIZE} (From {USED_DIR_SIZE}x{USED_DIR_SIZE})\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel('noise level')\n",
    "ax.set_ylabel('model accuracy')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "i = 1\n",
    "\n",
    "fig = plt.figure(num=2, facecolor='w', edgecolor='k')\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(20)\n",
    "subplots_adjust(hspace=-0.5, wspace=0.4)\n",
    "for level in prediction_data:\n",
    "    result = prediction_data[level]\n",
    "    labels = 'Match', 'Not match'\n",
    "    sizes = [float(result[0])/result[1], float(result[1] - result[0])/result[1]]  # Percentages\n",
    "    colors = ['green', 'red']\n",
    "    explode = (0, 0)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i)\n",
    "    i += 1\n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    ax.set_title(f'Noise = {level}')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IIAI-CEPDaYsW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
